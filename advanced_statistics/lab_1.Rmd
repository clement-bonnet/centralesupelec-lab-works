---
title: "LAB_1"
author: "Pierre Houdoin, Clément Bonnet"
date: "15/10/2020"
output: html_document
---

# Lab 1

# ML estimation with PDF

## Theoretical analysis

### Question 1: maximum likelihood estimator?

For n iid observations $x_i$ of the height of the river, the likelihood can be written as following
$$
\begin{align}
L(a;x_1,...,x_n)
&= \prod_{i=1}^nf_H(x_i) \\
&= \frac{1}{a^n} \Big(\prod_{i=1}^nx_i\Big) e^{-\frac{1}{2a}\sum_{i=1}^nx_i^2} \\
\end{align}
$$

The log-likelihood can be derived from the likelihood as follows
$$
\begin{align}
l(a;x_1,...,x_n)
&= log(L(a;x_1,...,x_n)) \\
&= \sum_{i=1}^n\ln(x_i) - n\ln(a) - \frac{1}{2a}\sum_{i=1}^nx_i^2 \\
\frac{\partial l}{\partial a}(a;x_1,...,x_n)
&= -\frac{n}{a} + \frac{1}{2a^2}\sum_{i=1}^nx_i^2
\end{align}
$$

Then one can derive the maximum likelihood estimator by setting the partial derivative to 0.
$$
\frac{\partial l}{\partial a}(\hat a_n;x_1,...,x_n) = 0
\iff \boxed{\hat a_n = \frac{1}{2n}\sum_{i=1}^nx_i^2}
$$


### Question 2: method of moments estimator?

$$
\begin{align}
E[X] &= \int_0^{+\infty}xf_H(x)dx \\
&= \int_0^{+\infty}\frac{x^2}{a}e^{\frac{-x^2}{2a}}dx \\
&= \int_0^{+\infty}e^{\frac{-x^2}{2a}}dx \\
&= \sqrt{\frac{\pi a}{2}}
\end{align}
$$

One can estimate the expectation using the arithmetic mean. Hence, the method of moments estimator $\bar a_n$ is:
$$
\frac{1}{n}\sum_{i=1}^nx_i = \sqrt{\frac{\pi \bar a_n}{2}} \\
\iff \boxed{\bar a_n = \frac{2}{\pi n^2}\Big(\sum_{i=1}^nx_i\Big)^2}
$$


### Question 3: properties of $\hat a_n$?

#### a) Unbiased?

$$
\begin{align}
E[\hat a_n] 
&= \frac{1}{2n}\sum_{i=1}^nE[X_i^2] \\
&= \frac{1}{2}E[X^2] \\
&= \frac{1}{2}\int_0^{+\infty}\frac{x^3}{a}e^{\frac{-x^2}{2a}}dx \\
&= \int_0^{+\infty}xe^{\frac{-x^2}{2a}}dx
\end{align}
$$
$$\boxed{E[\hat a_n] = a}$$

$\hat a_n$ is unbiased.


#### b) Optimal?

Let us derive the variance of the estimator $\hat a_n$.
$$
\begin{align}
Var[\hat a_n] 
&= \frac{1}{4n^2}\sum_{i=1}^nVar[X^2] \\
&= \frac{1}{4n}\big(E[X^4] - E[X^2]^2\big) \\
\\
E[X^4]
&= \int_0^{+\infty}\frac{x^5}{a}e^{\frac{-x^2}{2a}}dx \\
&= 4\int_0^{+\infty}x^3e^{\frac{-x^2}{2a}}dx \\
&= 4aE[X^2] \\
Thus, Var[\hat a_n] 
&= \frac{1}{4n}\big(4aE[X^2] - E[X^2]^2\big) \\
\end{align}
$$
We know from question $a)$ that $E[X^2]=2E[\hat a_n]=2a$.
$$
\boxed{Var[\hat a_n] = \frac{a^2}{n}}
$$

Let us now compute the Fisher information $I(a)$
$$
\begin{align}
\frac{\partial^2}{\partial a^2}l(a;x_1,...,x_n)
&= \frac{n}{a^2} - \frac{1}{a^3}\sum_{i=1}^nx_i^2 \\
\\
I_n(a) 
&= E[-\frac{\partial^2}{\partial a^2}l(a;x_1,...,x_n)] \\
&= \frac{n}{a^2}\big(-1 + \frac{1}{a}E[X^2]\big) \\
I_n(a)
&= \frac{n}{a^2}
\end{align}
$$
$$
\boxed{Var[\hat a_n] = \frac{1}{I_n(a)}}
$$

Its variance equals the Cramer–Rao lower bound and it is unbiased. Hence, $\hat a_n$ minimizes the mean squared error. So $\hat a_n$ is both optimal.

#### c) Efficient?

Since $\hat a_n$ is unbiased and optimal. Therefore, $\hat a_n$ is efficient because its variance is equal to the Cramer-Rao lower bound.


#### d) Asymptotically Gaussian?

The maximum likelihood estimator is asymptotically gaussian. Hence, $\hat a_n$ is asymptotically gaussian. $I(a) = \frac{I_n(a)}{n} = \frac{1}{a^2}$
$$
\boxed{\sqrt{n}(\hat a_n - a) \xrightarrow[n \rightarrow \infty]{d} N\big(0,a^2 \big)}
$$


## Application on real data

### Question 1: p function of a?

Let $p$ the probability that a disaster happens during one year.
$$
\begin{align}
p 
&= 1 - F_H(6) \\
&= \int_6^\infty\frac{x}{a}e^{-\frac{x^2}{2a}}dx \\
&= \Big[-e^{-\frac{x^2}{2a}}\Big]_6^\infty \\
\end{align}
$$
$$
\boxed{p = e^{-\frac{18}{a}}}
$$

### Question 2: probability of at most one disaster?

During one thousand years, if at most one disaster happened, it means either there was no disasters, or there was only one.
Let us derive $p_1$, the probability that at most one disaster happens during one thousand years.
$$
\boxed{p_1 = (1-p)^{999} = (1-e^{-\frac{18}{a}})^{999}}
$$


### Question 3: estimation of the probability?


```{r}
X = c(2.5, 1.8, 2.9, 0.9, 2.1, 1.7, 2.2, 2.8)
n = length(X)
a = sum(X^2)/(2*n)
p = (1-exp(-18/a))^999
```

Regarding the set of 8 observations, one can estimate $\hat a = `r round(a,digits=2)`$. The probability $p_1$ can be estimated: $\boxed{p_1 = `r round(p,digits=3)`}$.



# Exercise 1: Rayleigh distribution

### (a)

The parameter $a$ of the Rayleigh distribution was estimated with the maximum likelihood estimator $\hat a_n$. It was found that $ a \approx `r round(a,digits=2)`$.

### (b)

One can generate more samples following a Rayleigh distribution by using the Rayleigh distribution function implemented in R. One has to be careful that the scale $\sigma$ used in R corresponds to $\sqrt{a}$. 
```{r echo=FALSE, message=FALSE, warning=FALSE}
library("VGAM")
```
```{r}
n = 100000
X = rrayleigh(n, scale=sqrt(a))
hist(X, breaks=12, xlim = c(0,10), main = "Rayleigh implemented in R")
```

If one has only access to uniform distribution and would like to output a Rayleigh distribution, one can use the inverse distribution function.
$$
\begin{align}
F(x)
&= \int_0^x\frac{t}{a}e^{-\frac{t^2}{2a}}dt \\
&= 1 - e^{-\frac{x^2}{2a}} \\
\\
F(x) = u \iff x = \sqrt{-2a \ln(1-u)}
\end{align}
$$

If $U$ follows a uniform distribution on $[0,1]$, one can generate samples following the Rayleigh distribution using the uniform distribution.
$$
U \sim U[0,1] \implies \sqrt{-2a \ln(U)} \sim Rayleigh(a)
$$

```{r}
n = 100000
U = runif(n)
X = sqrt(-2*a*log(U))
hist(X, breaks=12, xlim = c(0,10), main = "Simulated Rayleigh")
```

### (c)

Empirically, one can verify that the MLE is unbiased by averaging $N$ samples of the MLE $\hat a_{n,1},..., \hat a_{n,N}$ with whatever value for $n$. For computing resources reasons, let's take $n=10$ and average over $N=100000$ samples of $n$ observations.
$$
\begin{align}
E[\hat a_n - a] 
&\approx \frac{1}{N}\sum_{k=1}^N (\hat a_{n,k} - a) \\
&= \frac{1}{N}\sum_{k=1}^N (\frac{1}{2n}\sum_{i=1}^nx_{i,k}^2 - a)
\end{align}
$$

```{r}
N = 100000
n = 10
E = 0
for (k in 1:N){
  X = rrayleigh(n, scale=sqrt(a))
  E = E + 1/(2*n)*sum(X^2) - a
}
E = E/N
E
```
$\frac{E[\hat a_n - a]}{a} \approx `r round(E/a, digits=5)` \ll 1$. Hence, empirically, the estimator is unbiased.


### (d)

Empirically, one can verify the efficiency of the MLE estimator by computing its variance and compare it to the inverse of the Fisher information. One needs an unbiased estimator of the variance, knowing that the mean is $a$.
$$
\begin{align}
Var[\hat a_n] 
&\approx \frac{1}{N}\sum_{k=1}^N (\hat a_{n,k} - a)^2 \\
&= \frac{1}{N}\sum_{k=1}^N (\frac{1}{2n}\sum_{i=1}^nx_{i,k}^2 - a)^2
\end{align}
$$
```{r}
N = 100000
n = 10
I = n/a^2  # Fisher information
V = 0
for (k in 1:N){
  X = rrayleigh(n, scale=sqrt(a))
  dV = 1/(2*n)*sum(X^2) - a
  V = V + dV^2
}
V = V/N
V
```
$$
\frac{1}{I_n(a)} = \frac{a^2}{n} \approx `r round(1/I, digits=4)`
$$
$$
\frac{Var[\hat a_n] - \frac{1}{I_n(a)}}{\frac{1}{I_n(a)}} \approx `r round(I*(V-1/I), digits=5)` \ll 1
$$
Hence, one can say that $Var[\hat a_n] = \frac{1}{I_n(a)}$ and the estimator is efficient empirically.


### (e)

$$
\boxed{\sqrt{n}(\hat a_n - a) \xrightarrow[n \rightarrow \infty]{d} N\big(0,a^2 \big)}
$$
The asymptotic normality means that for $n$ large, $\sqrt{n}(\hat a_n - a) \sim N\big(0,a^2 \big)$. Thus one can plot several samples of the random variable $Z_n = \sqrt{n}(\hat a_n - a)$ and check whether the distribution looks Gaussian.
```{r}
n = 10000 # Size of the observations for each a_n
N = 5000   # Number of samples of Z_n
Z_n = rep(0,N)
for (k in 1:N){
  X = rrayleigh(n, scale=sqrt(a))
  a_n = 1/(2*n)*sum(X^2)
  Z_n[k] = sqrt(n)*(a_n - a)
}
hist(Z_n, breaks=12, main = "Asymptotic normality")
```

The MLE estimator is asymptotically normal. One can verify the standard deviation $a = `r round(a, digits=3)`$.